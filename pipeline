En primer lugar, ejecutar el script preprocessor.py para realizar data cleaning sobre los textos originales
presentes en el directorio raw_texts y almacenarlos en el directorio training_corpus. Los almacena de la forma esperada
por los algoritmos de entrenamiento de Gensim. Optativamente, se pueden utilizar los scripts trim_head y trim_tail para eliminar las n primeras líneas de un archivo txt ó para quedarse con las n primeras líneas de un archivo txt. En el archivo instructivo_trim_archivos se indica cómo invocar a los scripts con los parámetros pertinentes.

Luego ejecutar el script split_files.sh para fragmentar los textos crudos 
de modo que en los posteriores procesamientos no se generen documentos tan extensos
y así evitar problemas de alocación de memoria (los taggeos de SpaCy generan estructuras de datos
muy pesadas). 
Los textos, una vez fragmentados, deben estar ubicados en diferentes carpetas dentro del directorio raw_texts.
Cuanto más divididos estén los textos originales en diferentes carpetas, más eficiente será el procesamiento. 
20 fragmentos por directorio es el número recomendado.
Esto es así porque cuando se ejecute posteriormente el método tag_files de la clase BasicTagger, mantendrá en memoria todos los documentos 
de cada carpeta y solo al finalizar el procesamiento de los textos de una carpeta los guardará dentro de ella 
en un archivo binario que contendrá un objeto de tipo DocBin. 

Los objetos de la clase Chunk_Extractor, mediante el método process_files, serán los encargados de llevar a memoria todos los DocBins de 
una carpeta determinada (se indica por parámetro) y, una vez transformados en los docs originales,
realizar el procesamiento indicado. Utilizará objetos
de las distintas clases de Filters para realizar los diversos procesamientos. Los resultados serán estructuras de datos que pueden almacenarse como pickles ó en una base de datos, para que después se utilicen como sugerencias dentro del proceso generativo. 

Los objetos de la clase Dynamic_Generator tienen la responsabilidad de extender una secuencia determinada. Es la clase core del sistema. La API debería hacerle peticiones solamente a objetos de esta clase, salvo que se dé al usuario la posibilidad de elaborar su propio conjunto de datos a partir de los documentos almacenados como DocBins, en ese caso también se realizarían peticiones a objetos de la clase Chunk_Extractor.


El uso esperado es el de un asistente para la escritura, en el que se le pidan fragmentos del texto en construcción al sistema pero se espera que el usuario también contribuya en la elaboración del texto mediante su propia escritura. 
El proyecto final comprende el desarrollo de una interfaz, desde la que el usuario pueda escribir el texto con la asistencia de este sistema. Eventualmente también podría desarrollarse la funcionalidad para que el usuario establezca desde la interfaz su propio corpus, adjuntando archivos de extensión txt al sistema para que el se realice el preprocesamiento y procesamiento de ese corpus, estableciendo tanto los DocBins del sistema como el modelo de Gensim a partir de esos archivos txt enviados por el usuario.  


